"""
Comprehensive Evaluation Framework for Adaptive Preprocessing
Focus: Quantitative metrics, visual comparisons, and thorough analysis
"""

import numpy as np
import cv2
import matplotlib.pyplot as plt
from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr
from skimage import feature, filters, measure
import pandas as pd
import seaborn as sns
from scipy.stats import ttest_rel, wilcoxon
import warnings
warnings.filterwarnings('ignore')
from static_preprocessing import StaticPreprocessor
from adaptive_algo import AdaptivePreprocessor

from analyze import ImageQualityAnalyzer
from handle import DICOMHandler

class ComprehensiveEvaluationFramework:
    """
    Comprehensive evaluation system for comparing preprocessing approaches
    Focus: Original vs Static vs Adaptive comparison with quantitative metrics
    """
    
    def __init__(self):
        self.quality_analyzer = ImageQualityAnalyzer()
        self.dicom_handler = DICOMHandler()
        
        # Define evaluation metrics
        self.metrics_config = {
            'image_quality': {
                'brightness_optimality': {'target': 130, 'weight': 0.2, 'description': 'Closeness to optimal brightness (130)'},
                'contrast_adequacy': {'target': 50, 'weight': 0.25, 'description': 'RMS contrast adequacy'},
                'sharpness_level': {'target': 300, 'weight': 0.25, 'description': 'Laplacian variance (sharpness)'},
                'noise_quality': {'target': 25, 'weight': 0.3, 'description': 'Signal-to-noise ratio (dB)'}
            },
            'downstream_tasks': {
                'edge_detection_quality': {'weight': 0.4, 'description': 'Edge detection effectiveness'},
                'feature_detection_count': {'weight': 0.3, 'description': 'SIFT feature detection count'},
                'texture_analysis_quality': {'weight': 0.3, 'description': 'Texture analysis clarity'}
            },
            'enhancement_metrics': {
                'detail_preservation': {'description': 'How well fine details are preserved'},
                'artifact_introduction': {'description': 'Presence of processing artifacts'},
                'overall_enhancement': {'description': 'Overall visual improvement'}
            }
        }
        
        self.results_database = []
        
        print("Comprehensive Evaluation Framework initialized")
        print("Metrics: Image quality, downstream tasks, enhancement assessment")
    
    
    
    def calculate_image_quality_metrics(self, image):
        """
        Calculate comprehensive image quality metrics
        """
        # Use existing quality analyzer
        metrics = self.quality_analyzer.compute_comprehensive_metrics(image)
        
        # Extract key values
        brightness = metrics['brightness']['mean_intensity']
        contrast = metrics['contrast']['rms_contrast']
        sharpness = metrics['sharpness']['laplacian_variance']
        snr = metrics['noise']['estimated_snr_db']
        
        # Calculate normalized scores (0-100)
        brightness_score = max(0, 100 - abs(brightness - 130) / 130 * 100)
        contrast_score = min(100, contrast / 50 * 100)
        sharpness_score = min(100, sharpness / 500 * 100)
        snr_score = min(100, max(0, snr) / 30 * 100)
        
        # Overall quality score
        overall_quality = (
            brightness_score * 0.2 + 
            contrast_score * 0.25 + 
            sharpness_score * 0.25 + 
            snr_score * 0.3
        )
        
        return {
            'brightness': brightness,
            'contrast': contrast,
            'sharpness': sharpness,
            'snr': snr,
            'brightness_score': brightness_score,
            'contrast_score': contrast_score,
            'sharpness_score': sharpness_score,
            'snr_score': snr_score,
            'overall_quality': overall_quality
        }
    
    def calculate_downstream_task_metrics(self, image):
        """
        Calculate metrics relevant to downstream tasks
        """
        metrics = {}
        
        # 1. Edge Detection Quality
        # Apply Canny edge detection and measure edge quality
        edges = cv2.Canny(image, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        # Edge strength
        sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)
        edge_strength = np.mean(np.sqrt(sobel_x**2 + sobel_y**2))
        
        # Combine edge density and strength
        edge_quality = (edge_density * 100 + edge_strength / 10) / 2
        metrics['edge_detection_quality'] = min(100, edge_quality)
        
        # 2. Feature Detection (SIFT)
        try:
            sift = cv2.SIFT_create(nfeatures=500)
            keypoints, descriptors = sift.detectAndCompute(image, None)
            feature_count = len(keypoints)
            # Normalize to 0-100 scale (assuming 0-200 features is typical range)
            metrics['feature_detection_count'] = min(100, feature_count / 2)
        except:
            metrics['feature_detection_count'] = 0
        
        # 3. Texture Analysis Quality
        # Use Local Binary Pattern to assess texture clarity
        try:
            lbp = feature.local_binary_pattern(image, 8, 1, method='uniform')
            lbp_var = np.var(lbp)
            # Normalize texture variance
            metrics['texture_analysis_quality'] = min(100, lbp_var / 50)
        except:
            metrics['texture_analysis_quality'] = 50
        
        return metrics
    
    def calculate_enhancement_metrics(self, original, enhanced):
        """
        Calculate enhancement-specific metrics comparing original vs enhanced
        """
        metrics = {}
        
        # 1. Detail Preservation using SSIM
        try:
            detail_preservation = ssim(original, enhanced, data_range=255) * 100
            metrics['detail_preservation'] = detail_preservation
        except:
            metrics['detail_preservation'] = 50
        
        # 2. Artifact Introduction
        # Detect potential artifacts by looking at high-frequency differences
        orig_laplacian = cv2.Laplacian(original, cv2.CV_64F)
        enh_laplacian = cv2.Laplacian(enhanced, cv2.CV_64F)
        
        # Calculate difference in high-frequency content
        hf_diff = np.std(enh_laplacian - orig_laplacian)
        # Lower artifact score means fewer artifacts (inverted scale)
        artifact_score = max(0, 100 - hf_diff / 10)
        metrics['artifact_introduction'] = min(100, artifact_score)
        
        # 3. Overall Enhancement
        # Compare quality scores
        orig_quality = self.calculate_image_quality_metrics(original)
        enh_quality = self.calculate_image_quality_metrics(enhanced)
        
        quality_improvement = enh_quality['overall_quality'] - orig_quality['overall_quality']
        # Normalize to 0-100 scale (0 = worse, 50 = no change, 100 = much better)
        enhancement_score = 50 + quality_improvement
        metrics['overall_enhancement'] = np.clip(enhancement_score, 0, 100)
        
        return metrics
    
    def evaluate_single_image(self, original_image, static_processed, adaptive_processed, filename=""):
        """
        Comprehensive evaluation of a single image across all methods
        """
        print(f"Evaluating {filename}...")
        
        results = {
            'filename': filename,
            'methods': {
                'original': {'image': original_image},
                'static': {'image': static_processed},
                'adaptive': {'image': adaptive_processed}
            }
        }
        
        # Calculate metrics for each method
        for method_name, method_data in results['methods'].items():
            image = method_data['image']
            
            # Image quality metrics
            quality_metrics = self.calculate_image_quality_metrics(image)
            method_data['quality_metrics'] = quality_metrics
            
            # Downstream task metrics
            task_metrics = self.calculate_downstream_task_metrics(image)
            method_data['task_metrics'] = task_metrics
            
            # Enhancement metrics (compare to original)
            if method_name != 'original':
                enhancement_metrics = self.calculate_enhancement_metrics(original_image, image)
                method_data['enhancement_metrics'] = enhancement_metrics
            else:
                method_data['enhancement_metrics'] = {
                    'detail_preservation': 100,
                    'artifact_introduction': 100,
                    'overall_enhancement': 50  # Baseline
                }
        
        return results
    
    def evaluate_complete_dataset(self, images_dir="Images", adaptive_preprocessor=None):
        """
        Evaluate complete dataset comparing Original vs Static vs Adaptive
        """
        print("COMPREHENSIVE DATASET EVALUATION")
        print("="*60)
        print("Comparing: Original â†’ Static â†’ Adaptive")
        print("Metrics: Quality, Downstream Tasks, Enhancement")
        
        # Load images
        processed_data = self.dicom_handler.process_images_directory(images_dir)
        if not processed_data:
            print("No images found for evaluation")
            return None
        
        # Initialize static pipeline
        static_pipeline = StaticPreprocessor()
        
        # Process all images
        all_results = []
        
        for i, (filename, original_image, metadata) in enumerate(processed_data):
            print(f"\nProcessing {i+1}/{len(processed_data)}: {filename}")
            
            # Apply static preprocessing
            static_result, _ = static_pipeline.static_preprocessing_pipeline(original_image, show_steps=False)
            
            # Apply adaptive preprocessing
            if adaptive_preprocessor:
                adaptive_result = adaptive_preprocessor.process_image(original_image, show_steps=False)
                adaptive_image = adaptive_result[0] if isinstance(adaptive_result, tuple) else adaptive_result
            else:
                # Fallback: use static result
                adaptive_image = static_result
                print("  Warning: No adaptive preprocessor provided, using static result")
            
            # Evaluate this image
            image_results = self.evaluate_single_image(
                original_image, static_result, adaptive_image, filename
            )
            
            all_results.append(image_results)
            
            # Print brief summary
            orig_quality = image_results['methods']['original']['quality_metrics']['overall_quality']
            static_quality = image_results['methods']['static']['quality_metrics']['overall_quality']
            adaptive_quality = image_results['methods']['adaptive']['quality_metrics']['overall_quality']
            
            print(f"  Quality scores: Original={orig_quality:.1f}, Static={static_quality:.1f}, Adaptive={adaptive_quality:.1f}")
        
        # Store results
        self.results_database = all_results
        
        # Generate comprehensive analysis
        self.generate_quantitative_analysis()
        self.create_visual_comparisons()
        self.analyze_advantages_limitations()
        
        return all_results
    
    def generate_quantitative_analysis(self):
        """
        Generate detailed quantitative analysis and statistics
        """
        print("\n" + "="*60)
        print("QUANTITATIVE EVALUATION RESULTS")
        print("="*60)
        
        if not self.results_database:
            print("No results to analyze")
            return
        
        # Prepare data for analysis
        methods = ['original', 'static', 'adaptive']
        metrics_data = {method: {} for method in methods}
        
        # Collect all metrics
        for result in self.results_database:
            for method in methods:
                method_data = result['methods'][method]
                
                # Quality metrics
                for metric, value in method_data['quality_metrics'].items():
                    if metric not in metrics_data[method]:
                        metrics_data[method][metric] = []
                    metrics_data[method][metric].append(value)
                
                # Task metrics
                for metric, value in method_data['task_metrics'].items():
                    if metric not in metrics_data[method]:
                        metrics_data[method][metric] = []
                    metrics_data[method][metric].append(value)
                
                # Enhancement metrics
                for metric, value in method_data['enhancement_metrics'].items():
                    if metric not in metrics_data[method]:
                        metrics_data[method][metric] = []
                    metrics_data[method][metric].append(value)
        
        # Create comparison table
        self.create_comparison_table(metrics_data)
        
        # Statistical significance testing
        self.perform_statistical_tests(metrics_data)
        
        # Performance summary
        self.summarize_performance(metrics_data)
    
    def create_comparison_table(self, metrics_data):
        """Create detailed comparison table"""
        print("\nDETAILED METRICS COMPARISON")
        print("-" * 80)
        
        # Key metrics to display
        key_metrics = [
            'overall_quality', 'brightness_score', 'contrast_score', 'sharpness_score', 'snr_score',
            'edge_detection_quality', 'feature_detection_count', 'texture_analysis_quality',
            'detail_preservation', 'artifact_introduction', 'overall_enhancement'
        ]
        
        print(f"{'Metric':<25} {'Original':<12} {'Static':<12} {'Adaptive':<12} {'Best':<10}")
        print("-" * 80)
        
        for metric in key_metrics:
            if metric in metrics_data['original']:
                orig_mean = np.mean(metrics_data['original'][metric])
                static_mean = np.mean(metrics_data['static'][metric])
                adaptive_mean = np.mean(metrics_data['adaptive'][metric])
                
                # Determine best method
                values = {'Original': orig_mean, 'Static': static_mean, 'Adaptive': adaptive_mean}
                best_method = max(values, key=values.get)
                
                print(f"{metric:<25} {orig_mean:<12.2f} {static_mean:<12.2f} {adaptive_mean:<12.2f} {best_method:<10}")
    
    def perform_statistical_tests(self, metrics_data):
        """Perform statistical significance tests"""
        print(f"\nSTATISTICAL SIGNIFICANCE TESTS")
        print("-" * 50)
        
        # Test static vs adaptive for key metrics
        key_metrics = ['overall_quality', 'edge_detection_quality', 'overall_enhancement']
        
        for metric in key_metrics:
            if metric in metrics_data['static'] and metric in metrics_data['adaptive']:
                static_values = metrics_data['static'][metric]
                adaptive_values = metrics_data['adaptive'][metric]
                
                # Paired t-test
                try:
                    t_stat, t_pval = ttest_rel(adaptive_values, static_values)
                    w_stat, w_pval = wilcoxon(adaptive_values, static_values)
                    
                    print(f"\n{metric}:")
                    print(f"  Paired t-test: t={t_stat:.3f}, p={t_pval:.3f}")
                    print(f"  Wilcoxon test: W={w_stat:.3f}, p={w_pval:.3f}")
                    
                    if t_pval < 0.05:
                        print(f"  âœ“ Significant difference (p < 0.05)")
                    else:
                        print(f"  - No significant difference (p â‰¥ 0.05)")
                except:
                    print(f"  Could not perform statistical test for {metric}")
    
    def summarize_performance(self, metrics_data):
        """Summarize overall performance"""
        print(f"\nPERFORMANCE SUMMARY")
        print("-" * 30)
        
        # Calculate improvement rates
        static_improvements = 0
        adaptive_improvements = 0
        total_images = len(self.results_database)
        
        for result in self.results_database:
            orig_quality = result['methods']['original']['quality_metrics']['overall_quality']
            static_quality = result['methods']['static']['quality_metrics']['overall_quality']
            adaptive_quality = result['methods']['adaptive']['quality_metrics']['overall_quality']
            
            if static_quality > orig_quality:
                static_improvements += 1
            if adaptive_quality > orig_quality:
                adaptive_improvements += 1
        
        print(f"Images improved by static preprocessing: {static_improvements}/{total_images} ({static_improvements/total_images*100:.1f}%)")
        print(f"Images improved by adaptive preprocessing: {adaptive_improvements}/{total_images} ({adaptive_improvements/total_images*100:.1f}%)")
        
        # Average improvements
        static_avg_improvement = np.mean([
            result['methods']['static']['quality_metrics']['overall_quality'] - 
            result['methods']['original']['quality_metrics']['overall_quality']
            for result in self.results_database
        ])
        
        adaptive_avg_improvement = np.mean([
            result['methods']['adaptive']['quality_metrics']['overall_quality'] - 
            result['methods']['original']['quality_metrics']['overall_quality']
            for result in self.results_database
        ])
        
        print(f"Average quality improvement - Static: {static_avg_improvement:+.2f}")
        print(f"Average quality improvement - Adaptive: {adaptive_avg_improvement:+.2f}")
        
        # Best method overall
        if adaptive_avg_improvement > static_avg_improvement:
            print(f"ðŸ† Overall winner: Adaptive preprocessing ({adaptive_avg_improvement - static_avg_improvement:+.2f} points better)")
        else:
            print(f"ðŸ† Overall winner: Static preprocessing ({static_avg_improvement - adaptive_avg_improvement:+.2f} points better)")
    
    def create_visual_comparisons(self):
        """Create comprehensive visual comparisons"""
        print(f"\nCREATING VISUAL COMPARISONS")
        print("-" * 40)
        
        if not self.results_database:
            print("No results for visualization")
            return
        
        # Select representative images for visualization
        n_display = min(3, len(self.results_database))
        display_results = self.results_database[:n_display]
        
        # Create side-by-side comparison
        fig, axes = plt.subplots(n_display, 3, figsize=(15, 5*n_display))
        if n_display == 1:
            axes = axes.reshape(1, -1)
        
        methods = ['original', 'static', 'adaptive']
        method_titles = ['Original', 'Static Pipeline', 'Adaptive Pipeline']
        
        for i, result in enumerate(display_results):
            for j, (method, title) in enumerate(zip(methods, method_titles)):
                ax = axes[i, j] if n_display > 1 else axes[j]
                
                image = result['methods'][method]['image']
                quality = result['methods'][method]['quality_metrics']['overall_quality']
                
                ax.imshow(image, cmap='gray', vmin=0, vmax=255)
                ax.set_title(f'{title}\nQuality: {quality:.1f}', fontweight='bold', fontsize=12)
                ax.axis('off')
                
                # Add metrics text
                metrics_text = (
                    f"Brightness: {result['methods'][method]['quality_metrics']['brightness']:.1f}\n"
                    f"Contrast: {result['methods'][method]['quality_metrics']['contrast']:.1f}\n"
                    f"Sharpness: {result['methods'][method]['quality_metrics']['sharpness']:.1f}\n"
                    f"SNR: {result['methods'][method]['quality_metrics']['snr']:.1f}dB"
                )
                
                ax.text(0.02, 0.98, metrics_text, transform=ax.transAxes,
                       verticalalignment='top', fontsize=8,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.suptitle('Preprocessing Methods Comparison\n(Original vs Static vs Adaptive)', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # Create metrics comparison chart
        self.create_metrics_visualization()
    
    def create_metrics_visualization(self):
        """Create metrics comparison visualization"""
        # Collect average metrics
        methods = ['Original', 'Static', 'Adaptive']
        key_metrics = ['overall_quality', 'edge_detection_quality', 'detail_preservation', 'overall_enhancement']
        metric_labels = ['Overall Quality', 'Edge Detection', 'Detail Preservation', 'Enhancement Score']
        
        metrics_summary = {method: [] for method in methods}
        
        for metric in key_metrics:
            for i, method in enumerate(['original', 'static', 'adaptive']):
                if metric in self.results_database[0]['methods'][method]['quality_metrics']:
                    values = [result['methods'][method]['quality_metrics'][metric] for result in self.results_database]
                elif metric in self.results_database[0]['methods'][method]['task_metrics']:
                    values = [result['methods'][method]['task_metrics'][metric] for result in self.results_database]
                else:
                    values = [result['methods'][method]['enhancement_metrics'][metric] for result in self.results_database]
                
                metrics_summary[methods[i]].append(np.mean(values))
        
        # Create radar chart
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Bar chart
        x = np.arange(len(metric_labels))
        width = 0.25
        
        for i, method in enumerate(methods):
            ax1.bar(x + i*width, metrics_summary[method], width, label=method, alpha=0.8)
        
        ax1.set_xlabel('Metrics')
        ax1.set_ylabel('Score (0-100)')
        ax1.set_title('Average Performance Comparison')
        ax1.set_xticks(x + width)
        ax1.set_xticklabels(metric_labels, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Improvement over original
        static_improvements = [metrics_summary['Static'][i] - metrics_summary['Original'][i] for i in range(len(key_metrics))]
        adaptive_improvements = [metrics_summary['Adaptive'][i] - metrics_summary['Original'][i] for i in range(len(key_metrics))]
        
        ax2.bar(x - width/2, static_improvements, width, label='Static Improvement', alpha=0.8)
        ax2.bar(x + width/2, adaptive_improvements, width, label='Adaptive Improvement', alpha=0.8)
        ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
        
        ax2.set_xlabel('Metrics')
        ax2.set_ylabel('Improvement over Original')
        ax2.set_title('Improvement Analysis')
        ax2.set_xticks(x)
        ax2.set_xticklabels(metric_labels, rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def analyze_advantages_limitations(self):
        """Analyze advantages and limitations of each approach"""
        print("\n" + "="*60)
        print("ADVANTAGES AND LIMITATIONS ANALYSIS")
        print("="*60)
        
        if not self.results_database:
            print("No results to analyze")
            return
        
        # Analyze performance patterns
        static_wins = 0
        adaptive_wins = 0
        ties = 0
        
        static_strengths = []
        adaptive_strengths = []
        
        for result in self.results_database:
            static_quality = result['methods']['static']['quality_metrics']['overall_quality']
            adaptive_quality = result['methods']['adaptive']['quality_metrics']['overall_quality']
            
            if adaptive_quality > static_quality + 1:  # 1 point margin
                adaptive_wins += 1
                # Analyze what adaptive did better
                static_metrics = result['methods']['static']['quality_metrics']
                adaptive_metrics = result['methods']['adaptive']['quality_metrics']
                
                if adaptive_metrics['brightness_score'] > static_metrics['brightness_score']:
                    adaptive_strengths.append('brightness_correction')
                if adaptive_metrics['contrast_score'] > static_metrics['contrast_score']:
                    adaptive_strengths.append('contrast_enhancement')
                if adaptive_metrics['sharpness_score'] > static_metrics['sharpness_score']:
                    adaptive_strengths.append('sharpness_improvement')
                if adaptive_metrics['snr_score'] > static_metrics['snr_score']:
                    adaptive_strengths.append('noise_reduction')
                    
            elif static_quality > adaptive_quality + 1:
                static_wins += 1
                # Analyze what static did better
                static_metrics = result['methods']['static']['quality_metrics']
                adaptive_metrics = result['methods']['adaptive']['quality_metrics']
                
                if static_metrics['brightness_score'] > adaptive_metrics['brightness_score']:
                    static_strengths.append('brightness_correction')
                if static_metrics['contrast_score'] > adaptive_metrics['contrast_score']:
                    static_strengths.append('contrast_enhancement')
                if static_metrics['sharpness_score'] > adaptive_metrics['sharpness_score']:
                    static_strengths.append('sharpness_improvement')
                if static_metrics['snr_score'] > adaptive_metrics['snr_score']:
                    static_strengths.append('noise_reduction')
            else:
                ties += 1
        
        print(f"PERFORMANCE COMPARISON:")
        print(f"  Adaptive wins: {adaptive_wins}/{len(self.results_database)} ({adaptive_wins/len(self.results_database)*100:.1f}%)")
        print(f"  Static wins: {static_wins}/{len(self.results_database)} ({static_wins/len(self.results_database)*100:.1f}%)")
        print(f"  Ties: {ties}/{len(self.results_database)} ({ties/len(self.results_database)*100:.1f}%)")
        
        print(f"\nSTATIC PIPELINE ADVANTAGES:")
        print(f"  âœ“ Consistent and predictable results")
        print(f"  âœ“ Fast processing (no analysis overhead)")
        print(f"  âœ“ Simple implementation and maintenance")
        print(f"  âœ“ Reliable baseline performance")
        if static_strengths:
            from collections import Counter
            strength_counts = Counter(static_strengths)
            print(f"  âœ“ Particularly effective at: {', '.join([k for k, v in strength_counts.most_common(2)])}")
        
        print(f"\nSTATIC PIPELINE LIMITATIONS:")
        print(f"  âœ— Cannot adapt to image-specific characteristics")
        print(f"  âœ— May over-process or under-process certain images")
        print(f"  âœ— Fixed parameters may not be optimal for all cases")
        print(f"  âœ— Limited ability to handle varying quality levels")
        
        print(f"\nADAPTIVE PIPELINE ADVANTAGES:")
        print(f"  âœ“ Tailored processing for each image")
        print(f"  âœ“ Can handle diverse image quality levels")
        print(f"  âœ“ Optimizes processing order and parameters")
        print(f"  âœ“ Better theoretical potential for improvement")
        if adaptive_strengths:
            from collections import Counter
            strength_counts = Counter(adaptive_strengths)
            print(f"  âœ“ Particularly effective at: {', '.join([k for k, v in strength_counts.most_common(2)])}")
        
        print(f"\nADAPTIVE PIPELINE LIMITATIONS:")
        print(f"  âœ— More complex implementation and maintenance")
        print(f"  âœ— Longer processing time due to analysis overhead")
        print(f"  âœ— Potential for inconsistent results across similar images")
        print(f"  âœ— Requires careful tuning of decision thresholds")
        print(f"  âœ— May make suboptimal decisions with limited training data")
        
        # Recommendations
        print(f"\nRECOMMENDATIONS:")
        if adaptive_wins > static_wins:
            print(f"  ðŸŽ¯ Adaptive preprocessing shows {adaptive_wins - static_wins} more wins")
            print(f"  â†’ Recommend adaptive approach for quality-critical applications")
            print(f"  â†’ Consider static approach for high-throughput scenarios")
        else:
            print(f"  ðŸŽ¯ Static preprocessing shows competitive or better performance")
            print(f"  â†’ Consider if adaptive complexity is justified")
            print(f"  â†’ May need to refine adaptive decision logic")
        
        print(f"  ðŸ’¡ Hybrid approach: Use adaptive for difficult cases, static for routine processing")
        print(f"  ðŸ’¡ Monitor performance on larger, more diverse datasets")

def demonstrate_comprehensive_evaluation():
    """
    Demonstrate the comprehensive evaluation framework
    """
    print("COMPREHENSIVE EVALUATION DEMONSTRATION")
    print("="*50)
    
    # Initialize evaluation framework
    evaluator = ComprehensiveEvaluationFramework()
    
    # You would integrate this with your existing adaptive preprocessor
    # For demonstration, we'll create a simple adaptive preprocessor placeholder
    class SimpleAdaptivePreprocessor:
        def process_image(self, image, show_steps=False):
            # Simple adaptive logic: analyze and apply different processing
            mean_intensity = np.mean(image)
            contrast = np.std(image)
            sharpness = cv2.Laplacian(image, cv2.CV_64F).var()
            
            enhanced = image.copy()
            
            # Adaptive brightness correction
            if mean_intensity < 100:
                # Dark image - brighten with gamma correction
                normalized = enhanced.astype(np.float32) / 255.0
                enhanced = (np.power(normalized, 0.7) * 255).astype(np.uint8)
            elif mean_intensity > 180:
                # Bright image - darken with gamma correction
                normalized = enhanced.astype(np.float32) / 255.0
                enhanced = (np.power(normalized, 1.3) * 255).astype(np.uint8)
            
            # Adaptive contrast enhancement
            if contrast < 30:
                # Low contrast - strong CLAHE
                clahe = cv2.createCLAHE(clipLimit=3.5, tileGridSize=(8, 8))
                enhanced = clahe.apply(enhanced)
            elif contrast < 50:
                # Moderate contrast - mild CLAHE
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(6, 6))
                enhanced = clahe.apply(enhanced)
            
            # Adaptive noise reduction and sharpening
            if sharpness < 100:
                # Blurry image - light denoising + sharpening
                enhanced = cv2.GaussianBlur(enhanced, (3, 3), 0.3)
                kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) * 0.2
                kernel[1,1] = 1 + 0.2
                enhanced = cv2.filter2D(enhanced, -1, kernel)
                enhanced = np.clip(enhanced, 0, 255).astype(np.uint8)
            elif sharpness > 300:
                # Already sharp - just denoise
                enhanced = cv2.bilateralFilter(enhanced, 5, 25, 25)
            
            return enhanced
    
    # Create simple adaptive preprocessor for demo
    adaptive_preprocessor = SimpleAdaptivePreprocessor()
    
    # Run comprehensive evaluation
    print("\nRunning comprehensive evaluation...")
    print("This will:")
    print("1. Compare Original vs Static vs Adaptive on all images")
    print("2. Calculate comprehensive metrics")
    print("3. Perform statistical analysis")
    print("4. Generate visual comparisons")
    print("5. Analyze advantages and limitations")
    
    results = evaluator.evaluate_complete_dataset("Images", adaptive_preprocessor)
    
    return evaluator, results

class DetailedMetricsAnalyzer:
    """
    Additional detailed analysis for specific metrics and use cases
    """
    
    def __init__(self):
        self.quality_analyzer = ImageQualityAnalyzer()
    
    def analyze_diagnostic_suitability(self, original, processed, image_type="dental"):
        """
        Analyze how well processed image maintains diagnostic quality
        """
        diagnostic_metrics = {}
        
        # 1. Critical feature preservation
        # Detect and compare key features (edges, textures)
        orig_edges = cv2.Canny(original, 50, 150)
        proc_edges = cv2.Canny(processed, 50, 150)
        
        # Edge preservation ratio
        orig_edge_count = np.sum(orig_edges > 0)
        proc_edge_count = np.sum(proc_edges > 0)
        edge_preservation = proc_edge_count / orig_edge_count if orig_edge_count > 0 else 1.0
        diagnostic_metrics['edge_preservation_ratio'] = edge_preservation
        
        # 2. Fine detail preservation using high-frequency analysis
        orig_fft = np.fft.fft2(original)
        proc_fft = np.fft.fft2(processed)
        
        # High frequency correlation
        h, w = original.shape
        mask = np.zeros((h, w))
        center_h, center_w = h//2, w//2
        radius = min(h, w) // 4
        mask[center_h-radius:center_h+radius, center_w-radius:center_w+radius] = 1
        
        orig_hf = np.abs(orig_fft) * (1 - mask)
        proc_hf = np.abs(proc_fft) * (1 - mask)
        
        hf_correlation = np.corrcoef(orig_hf.flatten(), proc_hf.flatten())[0,1]
        diagnostic_metrics['high_freq_preservation'] = hf_correlation if not np.isnan(hf_correlation) else 0.0
        
        # 3. Artifact detection
        # Detect potential processing artifacts
        diff_image = cv2.absdiff(original, processed)
        artifact_level = np.std(diff_image)
        diagnostic_metrics['artifact_level'] = artifact_level
        
        # 4. Overall diagnostic quality score
        diagnostic_score = (
            edge_preservation * 0.4 +
            (hf_correlation if not np.isnan(hf_correlation) else 0.5) * 0.4 +
            max(0, 1 - artifact_level/50) * 0.2  # Normalize artifact level
        ) * 100
        
        diagnostic_metrics['overall_diagnostic_quality'] = min(100, diagnostic_score)
        
        return diagnostic_metrics
    
    def benchmark_against_reference(self, test_images, reference_method="original"):
        """
        Benchmark preprocessing methods against a reference standard
        """
        print(f"\nBENCHMARKING AGAINST {reference_method.upper()} REFERENCE")
        print("-" * 50)
        
        benchmarks = {
            'psnr_scores': [],
            'ssim_scores': [],
            'diagnostic_scores': []
        }
        
        for i, image_data in enumerate(test_images):
            if reference_method in image_data['methods']:
                reference = image_data['methods'][reference_method]['image']
                
                for method_name, method_data in image_data['methods'].items():
                    if method_name != reference_method:
                        test_image = method_data['image']
                        
                        # Calculate PSNR
                        psnr_score = psnr(reference, test_image, data_range=255)
                        
                        # Calculate SSIM
                        ssim_score = ssim(reference, test_image, data_range=255)
                        
                        # Calculate diagnostic quality
                        diag_metrics = self.analyze_diagnostic_suitability(reference, test_image)
                        diag_score = diag_metrics['overall_diagnostic_quality']
                        
                        benchmarks['psnr_scores'].append((method_name, psnr_score))
                        benchmarks['ssim_scores'].append((method_name, ssim_score))
                        benchmarks['diagnostic_scores'].append((method_name, diag_score))
        
        # Summarize benchmark results
        print(f"Average PSNR scores:")
        psnr_by_method = {}
        for method, score in benchmarks['psnr_scores']:
            if method not in psnr_by_method:
                psnr_by_method[method] = []
            psnr_by_method[method].append(score)
        
        for method, scores in psnr_by_method.items():
            print(f"  {method}: {np.mean(scores):.2f} Â± {np.std(scores):.2f} dB")
        
        print(f"\nAverage SSIM scores:")
        ssim_by_method = {}
        for method, score in benchmarks['ssim_scores']:
            if method not in ssim_by_method:
                ssim_by_method[method] = []
            ssim_by_method[method].append(score)
        
        for method, scores in ssim_by_method.items():
            print(f"  {method}: {np.mean(scores):.3f} Â± {np.std(scores):.3f}")
        
        return benchmarks
    
    def generate_detailed_report(self, evaluation_results, output_file="evaluation_report.txt"):
        """
        Generate a comprehensive evaluation report
        """
        with open(output_file, 'w') as f:
            f.write("COMPREHENSIVE PREPROCESSING EVALUATION REPORT\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("EXECUTIVE SUMMARY\n")
            f.write("-" * 20 + "\n")
            
            # Calculate summary statistics
            total_images = len(evaluation_results)
            static_improvements = sum(1 for result in evaluation_results 
                                    if result['methods']['static']['quality_metrics']['overall_quality'] > 
                                    result['methods']['original']['quality_metrics']['overall_quality'])
            adaptive_improvements = sum(1 for result in evaluation_results 
                                      if result['methods']['adaptive']['quality_metrics']['overall_quality'] > 
                                      result['methods']['original']['quality_metrics']['overall_quality'])
            
            f.write(f"Total images evaluated: {total_images}\n")
            f.write(f"Static preprocessing improvements: {static_improvements}/{total_images} ({static_improvements/total_images*100:.1f}%)\n")
            f.write(f"Adaptive preprocessing improvements: {adaptive_improvements}/{total_images} ({adaptive_improvements/total_images*100:.1f}%)\n\n")
            
            # Detailed results for each image
            f.write("DETAILED RESULTS BY IMAGE\n")
            f.write("-" * 30 + "\n")
            
            for result in evaluation_results:
                f.write(f"\nImage: {result['filename']}\n")
                f.write(f"{'Method':<12} {'Quality':<8} {'Brightness':<10} {'Contrast':<9} {'Sharpness':<9} {'SNR':<6}\n")
                f.write("-" * 60 + "\n")
                
                for method in ['original', 'static', 'adaptive']:
                    metrics = result['methods'][method]['quality_metrics']
                    f.write(f"{method:<12} {metrics['overall_quality']:<8.1f} "
                           f"{metrics['brightness']:<10.1f} {metrics['contrast']:<9.1f} "
                           f"{metrics['sharpness']:<9.1f} {metrics['snr']:<6.1f}\n")
            
            f.write(f"\nReport generated successfully: {output_file}")
        
        print(f"Detailed report saved to: {output_file}")

class AdvancedVisualization:
    """
    Advanced visualization capabilities for evaluation results
    """
    
    def create_performance_heatmap(self, evaluation_results):
        """Create heatmap showing performance across images and metrics"""
        # Prepare data
        images = [result['filename'] for result in evaluation_results]
        methods = ['Original', 'Static', 'Adaptive']
        metrics = ['Overall Quality', 'Brightness', 'Contrast', 'Sharpness', 'SNR']
        
        # Create data matrix
        data_matrix = []
        for method_key in ['original', 'static', 'adaptive']:
            method_data = []
            for result in evaluation_results:
                row = [
                    result['methods'][method_key]['quality_metrics']['overall_quality'],
                    result['methods'][method_key]['quality_metrics']['brightness_score'],
                    result['methods'][method_key]['quality_metrics']['contrast_score'],
                    result['methods'][method_key]['quality_metrics']['sharpness_score'],
                    result['methods'][method_key]['quality_metrics']['snr_score']
                ]
                method_data.append(row)
            data_matrix.append(method_data)
        
        # Create subplots for each method
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        for i, (method, method_data) in enumerate(zip(methods, data_matrix)):
            im = axes[i].imshow(method_data, cmap='viridis', aspect='auto', vmin=0, vmax=100)
            axes[i].set_title(f'{method} Method Performance', fontweight='bold')
            axes[i].set_xlabel('Metrics')
            axes[i].set_ylabel('Images')
            axes[i].set_xticks(range(len(metrics)))
            axes[i].set_xticklabels(metrics, rotation=45)
            axes[i].set_yticks(range(len(images)))
            axes[i].set_yticklabels([img[:10] + '...' if len(img) > 10 else img for img in images])
            
            # Add text annotations
            for y in range(len(images)):
                for x in range(len(metrics)):
                    text = f'{method_data[y][x]:.0f}'
                    axes[i].text(x, y, text, ha="center", va="center", 
                               color="white" if method_data[y][x] < 50 else "black", fontsize=8)
        
        # Add colorbar
        cbar = plt.colorbar(im, ax=axes, orientation='horizontal', pad=0.1, shrink=0.8)
        cbar.set_label('Performance Score (0-100)', fontweight='bold')
        
        plt.suptitle('Performance Heatmap Across Methods and Metrics', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.show()
    
    def create_improvement_analysis(self, evaluation_results):
        """Create detailed improvement analysis visualization"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Improvement distribution
        static_improvements = []
        adaptive_improvements = []
        
        for result in evaluation_results:
            orig_quality = result['methods']['original']['quality_metrics']['overall_quality']
            static_quality = result['methods']['static']['quality_metrics']['overall_quality']
            adaptive_quality = result['methods']['adaptive']['quality_metrics']['overall_quality']
            
            static_improvements.append(static_quality - orig_quality)
            adaptive_improvements.append(adaptive_quality - orig_quality)
        
        ax1.hist(static_improvements, bins=10, alpha=0.7, label='Static', color='blue')
        ax1.hist(adaptive_improvements, bins=10, alpha=0.7, label='Adaptive', color='red')
        ax1.axvline(0, color='black', linestyle='--', alpha=0.5)
        ax1.set_xlabel('Quality Improvement')
        ax1.set_ylabel('Number of Images')
        ax1.set_title('Distribution of Quality Improvements')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. Scatter plot: Static vs Adaptive improvements
        ax2.scatter(static_improvements, adaptive_improvements, alpha=0.7)
        ax2.plot([-10, 20], [-10, 20], 'k--', alpha=0.5)  # Diagonal line
        ax2.set_xlabel('Static Improvement')
        ax2.set_ylabel('Adaptive Improvement')
        ax2.set_title('Static vs Adaptive Improvement')
        ax2.grid(True, alpha=0.3)
        
        # Add quadrant labels
        ax2.text(5, -5, 'Static Better', ha='center', bbox=dict(boxstyle='round', facecolor='lightblue'))
        ax2.text(-5, 5, 'Adaptive Better', ha='center', bbox=dict(boxstyle='round', facecolor='lightcoral'))
        
        # 3. Method comparison by metric
        metrics = ['brightness_score', 'contrast_score', 'sharpness_score', 'snr_score']
        metric_labels = ['Brightness', 'Contrast', 'Sharpness', 'SNR']
        
        static_means = []
        adaptive_means = []
        original_means = []
        
        for metric in metrics:
            static_vals = [result['methods']['static']['quality_metrics'][metric] for result in evaluation_results]
            adaptive_vals = [result['methods']['adaptive']['quality_metrics'][metric] for result in evaluation_results]
            original_vals = [result['methods']['original']['quality_metrics'][metric] for result in evaluation_results]
            
            static_means.append(np.mean(static_vals))
            adaptive_means.append(np.mean(adaptive_vals))
            original_means.append(np.mean(original_vals))
        
        x = np.arange(len(metric_labels))
        width = 0.25
        
        ax3.bar(x - width, original_means, width, label='Original', alpha=0.8)
        ax3.bar(x, static_means, width, label='Static', alpha=0.8)
        ax3.bar(x + width, adaptive_means, width, label='Adaptive', alpha=0.8)
        
        ax3.set_xlabel('Metrics')
        ax3.set_ylabel('Average Score')
        ax3.set_title('Average Performance by Metric')
        ax3.set_xticks(x)
        ax3.set_xticklabels(metric_labels)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. Processing time comparison (simulated)
        methods = ['Static', 'Adaptive']
        times = [0.5, 2.1]  # Simulated processing times in seconds
        colors = ['blue', 'red']
        
        bars = ax4.bar(methods, times, color=colors, alpha=0.7)
        ax4.set_ylabel('Processing Time (seconds)')
        ax4.set_title('Processing Time Comparison')
        ax4.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar, time in zip(bars, times):
            ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,
                    f'{time:.1f}s', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.show()

# Integration function to run complete evaluation
def run_complete_evaluation(adaptive_preprocessor=AdaptivePreprocessor(), images_dir="Images"):
    """
    Run the complete evaluation pipeline
    """
    print("RUNNING COMPLETE EVALUATION PIPELINE")
    print("="*60)
    
    # Initialize frameworks
    evaluator = ComprehensiveEvaluationFramework()
    metrics_analyzer = DetailedMetricsAnalyzer()
    visualizer = AdvancedVisualization()
    
    # Run main evaluation
    results = evaluator.evaluate_complete_dataset(images_dir, adaptive_preprocessor)
    
    if results:
        # Additional detailed analysis
        print("\nRunning detailed metrics analysis...")
        benchmarks = metrics_analyzer.benchmark_against_reference(results, "original")
        
        # Generate detailed report
        print("\nGenerating detailed report...")
        metrics_analyzer.generate_detailed_report(results)
        
        # Create advanced visualizations
        print("\nCreating advanced visualizations...")
        visualizer.create_performance_heatmap(results)
        visualizer.create_improvement_analysis(results)
        
        print("\nâœ… Complete evaluation finished!")
        print("Generated:")
        print("  â€¢ Quantitative analysis with statistical tests")
        print("  â€¢ Visual comparisons (Original vs Static vs Adaptive)")
        print("  â€¢ Performance heatmaps")
        print("  â€¢ Improvement analysis")
        print("  â€¢ Detailed evaluation report")
        print("  â€¢ Advantages and limitations analysis")
        
        return evaluator, results
    
    else:
        print("âŒ Evaluation failed - no results generated")
        return None, None

if __name__ == "__main__":
    evaluator, results = demonstrate_comprehensive_evaluation()
    
    if results:
        print("\n" + "="*60)
        print("EVALUATION COMPLETE - KEY FINDINGS:")
        print("="*60)
        print("âœ“ Quantitative metrics calculated")
        print("âœ“ Statistical significance tested") 
        print("âœ“ Visual comparisons generated")
        print("âœ“ Advantages and limitations analyzed")
    else:
        print("Please provide image directory and adaptive preprocessor for evaluation")